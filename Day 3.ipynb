{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56de550f-d05f-486b-aca6-14599c9a38a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c20ffd2e-5fc4-4cfb-b054-7ce76bb5a74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate() # Init SparkSession\n",
    "\n",
    "d = [[\"1\", \"sravan\", \"company 1\"],\n",
    "        [\"2\", \"ojaswi\", \"company 1\"], \n",
    "        [\"3\", \"rohith\", \"company 2\"],\n",
    "        [\"4\", \"sridevi\", \"company 1\"], \n",
    "        [\"5\", \"bobby\", \"company 1\"]]\n",
    "\n",
    "cols = ['ID', 'Name', 'Company']\n",
    "df = spark.createDataFrame(d, cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8abb2873-391e-464a-8485-1c734a928fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate() # Init SparkSession\n",
    "\n",
    "d1 = [[\"1\", \"45000\", \"IT\"],\n",
    "         [\"2\", \"145000\", \"Manager\"],\n",
    "         [\"6\", \"45000\", \"HR\"],\n",
    "         [\"5\", \"34000\", \"Sales\"]]\n",
    "\n",
    "cols = ['ID', 'salary', 'department']\n",
    "df1 = spark.createDataFrame(d1, cols)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d3f4cb-e855-4df4-8fc7-08773b5607ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('join_example').getOrCreate()\n",
    "\n",
    "# first DataFrame\n",
    "d1 = [(\"1\", \"sravan\", \"company 1\"),\n",
    "      (\"2\", \"ojaswi\", \"company 1\"),\n",
    "      (\"3\", \"rohith\", \"company 2\"),\n",
    "      (\"4\", \"sridevi\", \"company 1\"),\n",
    "      (\"5\", \"bobby\", \"company 1\")]\n",
    "cols1 = ['id', 'name', 'company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# second DataFrame\n",
    "d2 = [(\"1\", \"45000\", \"IT\"),\n",
    "      (\"2\", \"145000\", \"Manager\"),\n",
    "      (\"6\", \"45000\", \"HR\"),\n",
    "      (\"5\", \"34000\", \"Sales\")]\n",
    "cols2 = ['id', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.id == df2.id, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb784272-2c09-4ecc-9766-320e7bf38dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# first dataframe data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# second dataframe data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf3a35d-fc5d-4a9e-b965-1556328e875c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# First dataframe data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# Second dataframe data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"fullouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99aae01-37b2-4e0d-be2d-682f9ab3e273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# First dataframe data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"], \n",
    "      [\"2\", \"ojaswi\", \"company 1\"], \n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"], \n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# Second dataframe data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"], \n",
    "      [\"2\", \"145000\", \"Manager\"], \n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a10e1ca-64da-4e54-8879-e1739db1f035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# first dataframe\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# second dataframe\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"leftouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9bea38b-01e8-4bd2-ada6-bc73bb23c33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# Data for left dataframe\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"], \n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# Data for right dataframe\n",
    "d2 = [[\"1\", \"45000\", \"IT\"], \n",
    "      [\"2\", \"145000\", \"Manager\"], \n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2410c6ae-eec2-49e7-8be8-95b78990d468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# Data for left dataframe\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"], \n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# Data for right dataframe\n",
    "d2 = [[\"1\", \"45000\", \"IT\"], \n",
    "      [\"2\", \"145000\", \"Manager\"], \n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2954533-e5a1-44b5-949d-36666d7a8b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# employee data\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"], \n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "cols1 = ['ID', 'NAME', 'Company']\n",
    "df1 = spark.createDataFrame(d1, cols1)\n",
    "\n",
    "# salary & department data\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"], \n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "cols2 = ['ID', 'salary', 'department']\n",
    "df2 = spark.createDataFrame(d2, cols2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf2feba-e120-4518-b50d-5a3a61a3110b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "d1 = [[\"1\", \"sravan\", \"company 1\"],\n",
    "      [\"2\", \"ojaswi\", \"company 1\"],\n",
    "      [\"3\", \"rohith\", \"company 2\"],\n",
    "      [\"4\", \"sridevi\", \"company 1\"],\n",
    "      [\"5\", \"bobby\", \"company 1\"]]\n",
    "col1 = ['ID', 'NAME', 'COMPANY']\n",
    "df1 = spark.createDataFrame(d1, col1)\n",
    "\n",
    "d2 = [[\"1\", \"45000\", \"IT\"],\n",
    "      [\"2\", \"145000\", \"Manager\"],\n",
    "      [\"6\", \"45000\", \"HR\"],\n",
    "      [\"5\", \"34000\", \"Sales\"]]\n",
    "col2 = ['ID', 'SALARY', 'DEPT']\n",
    "df2 = spark.createDataFrame(d2, col2)\n",
    "\n",
    "df1.join(df2, df1.ID == df2.ID, \"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74a5af9-b600-4d0b-b58c-40bb3a26d2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Top 5 products by revenue\n",
    "revenue = events.filter(F.col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"product_id\", \"product_name\") \\\n",
    "    .agg(F.sum(\"price\").alias(\"revenue\")) \\\n",
    "    .orderBy(F.desc(\"revenue\")).limit(5)\n",
    "\n",
    "# Running total per user\n",
    "window = Window.partitionBy(\"user_id\").orderBy(\"event_time\")\n",
    "events.withColumn(\"cumulative_events\", F.count(\"*\").over(window))\n",
    "\n",
    "# Conversion rate by category\n",
    "events.groupBy(\"category_code\", \"event_type\").count() \\\n",
    "    .pivot(\"event_type\").sum(\"count\") \\\n",
    "    .withColumn(\"conversion_rate\", F.col(\"purchase\")/F.col(\"view\")*100)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
